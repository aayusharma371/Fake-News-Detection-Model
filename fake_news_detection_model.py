# -*- coding: utf-8 -*-
"""Fake News Detection Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZTT3zNmcZLn6BGuxZsNtXBx4MnUOfZkC
"""

import pandas as pd
import numpy as np
import re, string
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import xgboost as xgb

def read_csv_manually(file_path):
    import csv
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        header = next(reader)
        data.append(header)
        for i, row in enumerate(reader):
            if len(row) == len(header):
                data.append(row)
            else:
                print(f"Skipping line {i+2} in {file_path}")
    return pd.DataFrame(data[1:], columns=data[0])

data_fake = read_csv_manually('/content/Fake.csv')
data_true = read_csv_manually('/content/True.csv')

for df in [data_fake, data_true]:
    df['title'] = df['title'].fillna('')
    df['text'] = df['text'].fillna('')

data_fake['label'] = 'fake'
data_true['label'] = 'true'

data_combine = pd.concat([data_fake, data_true], axis=0)
cols_to_drop = [c for c in ['subject','date'] if c in data_combine.columns]
data_combine = data_combine.drop(cols_to_drop, axis=1)
data_combine = data_combine.sample(frac=1).reset_index(drop=True)

def clean_text(text):
    text = str(text).lower()
    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

data_combine['title'] = data_combine['title'].apply(clean_text)
data_combine['text'] = data_combine['text'].apply(clean_text)
data_combine['text_combined'] = data_combine['title'] + ' ' + data_combine['text']

X = data_combine['text_combined']
y = data_combine['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)  # 'fake'->0, 'true'->1
y_test_enc = le.transform(y_test)

import xgboost as xgb

xgb_model = xgb.XGBClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    gamma=0,
    eval_metric='logloss',
    random_state=42,
    n_jobs=-1
)

xgb_model.fit(X_train_tfidf, y_train_enc)

print("XGBoost trained successfully (optimized & fast).")

y_pred_enc = xgb_model.predict(X_test_tfidf)
y_pred = le.inverse_transform(y_pred_enc)

accuracy = accuracy_score(y_test, y_pred)
print(f"\nAccuracy (XGBoost): {accuracy:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred, labels=['fake','true'])
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake','True'], yticklabels=['Fake','True'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - XGBoost')
plt.show()

data_fake_manual_testing = data_fake.tail(10).copy()
data_true_manual_testing = data_true.tail(10).copy()
manual_testing = pd.concat([data_fake_manual_testing, data_true_manual_testing], axis=0)
manual_testing['text_combined'] = (manual_testing['title'] + ' ' + manual_testing['text']).apply(clean_text)

X_manual = tfidf_vectorizer.transform(manual_testing['text_combined'])
y_manual_pred_enc = xgb_model.predict(X_manual)
y_manual_pred = le.inverse_transform(y_manual_pred_enc)

manual_testing_results = manual_testing.copy()
manual_testing_results['predicted_label'] = y_manual_pred
display(manual_testing_results[['title','text','label','predicted_label']])

fake_text = ' '.join(data_combine[data_combine['label']=='fake']['text_combined'])
true_text = ' '.join(data_combine[data_combine['label']=='true']['text_combined'])

wordcloud_fake = WordCloud(width=800, height=400, background_color='white', colormap='Reds').generate(fake_text)
plt.figure(figsize=(12,6))
plt.imshow(wordcloud_fake, interpolation='bilinear')
plt.axis('off')
plt.title('Most Frequent Words in Fake News')
plt.show()

wordcloud_true = WordCloud(width=800, height=400, background_color='white', colormap='Greens').generate(true_text)
plt.figure(figsize=(12,6))
plt.imshow(wordcloud_true, interpolation='bilinear')
plt.axis('off')
plt.title('Most Frequent Words in True News')
plt.show()